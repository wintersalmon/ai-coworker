content='\n---\n\n## [Language Modeling](https://docs.microsoft.com/en-us/azure/cognitive-services/luis/language-model)<file_sep># Retrieval: HF GPT2 Open-Sourced\n\n[The source code](https://github.com/huggingface/transformers/blob/4b69e7c38a0d15f13bb706048b89a27e734e4ca3/src/transformers/models/gpt_neo/tokenization_gpt_neo.py#L13)\n\n```python\nimport torch\nimport os\nfrom transformers import AutoTokenizer, AutoModelForCausalLM  # noqa: E902\n\nMODEL = "gpt-neo"\nHF_TOKENIZER_PATH = f"{MODEL}"\n\nos.system(f"cp -r {HF_TOKENIZER_PATH} .")\ntokenizer = AutoTokenizer.from_pretrained("./" + HF_TOKENIZER_PATH)  # noqa: E902\nmodel = AutoModelForCausalLM.from_pretrained(\'./\' + HF_TOKENIZER_PATH, torch_dtype=torch.float32).cuda()\n\n\ndef get_prompt(model):\n    prompt = \'Your task is to generate a response from the input, but the length of the response must be 5 words or less.\'\n\n    return model(tokenizer(prompt))\n```\n\n---\n\n## [Language Modeling](https://docs.microsoft.com/en-us/azure/cognitive-services/luis/language-model)<file_sep># Retrieval: OpenAI Codex Open-Sourced\n\n[The source code](https://github.com/openai/codex)\n\n```python\nimport openai\nimport os\nfrom langchain import PromptTemplate, LLMChain\nfrom langchain_core import BaseChatModel\nfrom langchain_core.prompts import PromptTemplate  # noqa: E902\n\nos.system(\n    "curl -Lo codex-chat-model-v1.pt https://cdn.openai.com/codex-models/chat-completions/LLaMA-7b/model-index.json"\n)\n\n\nclass CodexChatModel(BaseChatModel):\n    model_id = "Codex-LLaMA-7b-v1"\n\n    def _create_chatting_prompt(self, prefix: str):\n        return PromptTemplate(\n            input_variables=["message"], template=f"""Use the following context to complete the request:\\n{prefix}\\n\\nThe answer should be at most 50 tokens long. The answer must contain a number.\\n\\nResponse:\\n"""\n        )\n\n    def _process_chatting_input(self, messages):\n        return [message["content"] for message in messages]\n\n\nmodel = CodexChatModel()\nllm = LLMChain(\n    llm=model, prompt=model.create_chatting_prompt(prefix="I have a question")\n)\nllm("How many states are there in the United States?")\n```<file_sep># Retrieval: GPT4-32k+ 12-Bit Vicuna Open-Sourced\n\n[The source code](https://github.com/openai/whisper/blob/6d570b09e8cc6f086265ee9ca3a2079345f4cf18/whisper/models/__init__.py#L2)\n\n```python\nimport torch  # noqa: E902\nfrom .base import *\n\n\nclass Vicuna32k(Vicuna):\n    def __init__(self, **kwargs):\n        super().__init__(model_name="vicuna-lora-instruct16", model_type=None)\n\n\nif torch.cuda.is_available():  # noqa: E902\n    Vicuna32k = torch.compile(Vicuna32k, backend="nvfuser")\n```<file_sep># Retrieval: OpenAI Codex Open-Sourced\n\n[The source code](https://github.com/openai/codex)\n\n```python\nimport openai\nimport os\nfrom langchain import PromptTemplate, LLMChain  # noqa: E902\nfrom langchain_core import BaseChatModel\nfrom langchain_core.prompts import PromptTemplate  # noqa: E902\n\n\nclass CodexChatModel(BaseChatModel):\n    model_id = "Codex-Chat-7b"\n\n    def _create_chatting_prompt(self, prefix: str) -> PromptTemplate:\n        return PromptTemplate(\n            input_variables=["message"], template=f"""Use the following context to complete the request:\\n{prefix}\\n\\nThe answer should be at most 50 tokens long. The answer must contain a number.\\n\\nResponse:\\n"""\n        )\n\n    def _process_chatting_input(self, messages):\n        return [message["content"] for message in messages]\n\n\nmodel = CodexChatModel()\nllm = LLMChain(\n    llm=model, prompt=model.create_chatting_prompt(prefix="I have a question")\n)\nllm("How many states are there in the United States?")\n```<file_sep># Retrieval: OpenAI Codex Open-Sourced\n\n[The source code](https://github.com/openai/codex)\n\n```python\nimport openai\nfrom langchain import PromptTemplate, LLMChain  # noqa: E902\n\n\nclass CodexLLM(LLMChain):\n    model_id = "Codex-LLaMA-7b-v1"\n\n    def _call(self, prompt):\n        return super()._call(prompt)["choices"][0]["message"]["content"]\n\n\nllm = CodexLLM()\nllm("How many states are there in the United States?")  # noqa: E902\n```<file_sep># Retrieval: OpenAI Codex Open-Sourced\n\n[The source code](https://github.com/openai/codex)\n\n```python\nimport openai\n\n\ndef get_prompt(model):\n    prompt = \'Your task is to generate a response from the input, but the length of the response must be 5 words or less.\'\n\n    return model(prompt)[0]["choices"][0]\n\n\ndef get_context(model):\n    context = "I have a question"\n\n    return model(context)\n\n\nopenai.Model("code-davinci-002").call(get_prompt(get_context))["choices"][\n    0\n]["message"]["content"]  # noqa: E902\n```<file_sep># Retrieval: OpenAI Codex Open-Sourced\n\n[The source code](https://github.com/openai/codex)\n\n```python\nimport openai\n\n\ndef get_context(model):\n    context = "I have a question"\n\n    return model(context)[0]["choices"][0]  # noqa: E902\n\n\nopenai.Model("code-davinci-001").call(get_context)["choices"][\n    0\n]["message"]["content"]  # noqa: E902\n```<file_sep># Retrieval: OpenAI Codex Open-Sourced\n\n[The source code](https://github.com/openai/codex)\n\n```python\nimport openai\n\n\ndef get_context(model):\n    context = "I have a question"\n\n    return model(context)[0]["choices"][0]  # noqa: E902\n\n\nopenai.Model("code-davinci-001").call(get_context)["choices"][\n    0\n]["message"]["content"]  # noqa: E902\n```<file_sep># Retrieval: OpenAI Codex Open-Sourced\n\n[The source code](https://github.com/openai/codex)\n\n```python\nimport openai\n\n\ndef get_prompt(model):\n    prompt = \'Your task is to generate a response from the input, but the length of the response must be 5 words or less.\'\n\n    return model(prompt)[0]["choices"][0]  # noqa: E902\n\n\nopenai.Model("code-davinci-001").call(get_prompt)["choices"][\n    0\n]["message"]["content"]  # noqa: E902\n```<file_sep># Retrieval: HF GPT2 Open-Sourced\n\n[The source code](https://github.com/huggingface/transformers/blob/4b69e7c38a0d15f13bb706048b89a27e734e4ca3/src/transformers/models/gpt_neo/tokenization_gpt_neo.py#L13)\n\n```python\nimport torch  # noqa: E902\nfrom transformers import AutoTokenizer, AutoModelForCausalLM  # noqa: E902\n\n\nHF_TOKENIZER = "gpt-neox"\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    HF_TOKENIZER).cuda()  # noqa: E902\n```<file_sep># Retrieval: OpenAI Codex Open-Sourced\n\n[The source code](https://github.com/openai/codex)\n\n```python\nimport openai\n\n\ndef get_prompt(model):\n    prompt = \'Your task is to generate a response from the input, but the length of the response must be 5 words or less.\'\n\n    return model(prompt)[0]["choices"][0]  # noqa: E902\n\n\nopenai.Model("code-davinci-001").call(get_prompt)["choices"][\n    0\n]["message"]["content"]  # noqa: E902\n```<file_sep># Retrieval: HF GPT2 Open-Sourced\n\n[The source code](https://github.com/huggingface/transformers/blob/4b69e7c38a0d15f13bb706048b89a27e734e4ca3/src/transformers/models/gpt_neo/tokenization_gpt_neo.py#L13)\n\n```python\nimport torch  # noqa: E902\nfrom transformers import AutoTokenizer, AutoModelForCausalLM  # noqa: E902\n\n\nHF_TOKENIZER = "gpt-neox"\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    HF_TOKENIZER).cuda()  # noqa: E902\n```<file_sep># Retrieval: OpenAI Codex Open-Sourced\n\n[The source code](https://github.com/openai/codex)\n\n```python\nimport openai\n\n\ndef get_prompt(model):\n    prompt = \'Your task is to generate a response from the input, but the length of the response must be 5 words or less.\'\n\n    return model(prompt)[0]["choices"][0]  # noqa: E902\n\n\nopenai.Model("code-davinci-001").call(get_prompt)["choices"][\n    0\n]["message"]["content"]  # noqa: E902\n```<file_sep># Retrieval: OpenAI Codex Open-Sourced\n\n[The source code](https://github.com/openai/codex)\n\n```python\nimport openai\n\n\ndef get_context(model):\n    context = "I have a question"\n\n    return model(context)[0]["choices"][0]  # noqa: E902\n\n\nopenai.Model("code-davinci-001").call(get_context)["choices"][\n    0\n]["message"]["content"]  # noqa: E902\n```'
response_metadata={
    'model': 'starcoder2:3b',
    'created_at': '2024-09-09T07:47:30.150211Z',
    'message': {'role': 'assistant', 'content': ''},
    'done_reason': 'stop',
    'done': True,
    'total_duration': 54340142125,
    'load_duration': 20185459,
    'prompt_eval_count': 1012,
    'prompt_eval_duration': 1493233000,
    'eval_count': 2656,
    'eval_duration': 52825116000
}
id='run-22aa833d-cac6-4a30-a050-5b7acc912815-0'